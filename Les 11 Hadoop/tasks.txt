Нужно решить задачу при помощи map reduce подхода.
В файле будут описаны задания, и у кого какое задание, по желанию можете попробовать решить 4-ю задачу, нужно хорошо подумать там.
Подсказки: 
1) не всегда нужен map; 
2) shuffle делать при помощи команды sort (у нее есть дополнительные ключи, чтобы делать сортировку по нужным колонкам). 
Результат выполнения работы, это файлы на Python с решением задач, и shell команда для запуска задачи.

Нужно решить задачу при помощи map reduce подхода. 
В файле будут описаны задания, и у кого какое задание, по желанию можете попробовать решить 4-ю задачу, нужно хорошо подумать там.
Подсказки: 
1) не всегда нужен map; 
2) shuffle делать при помощи команды sort (у нее есть дополнительные ключи, чтобы делать сортировку по нужным колонкам).
Результат выполнения работы, это файлы на Python с решением задач, и shell команда для запуска задачи.

Нужно решить задачу при помощи map reduce подхода. 
В файле будут описаны задания, и у кого какое задание, по желанию можете попробовать решить 4-ю задачу, нужно хорошо подумать там.
Подсказки: 
1) не всегда нужен map; 
2) shuffle делать при помощи команды sort (у нее есть дополнительные ключи, чтобы делать сортировку по нужным колонкам).
Результат выполнения работы, это файлы на Python с решением задач, и shell команда для запуска задачи.


name	|	task
-----------------
Олег	|	1	
Борис	|	3
Егор	|	1
Анна	|	3
Валерия	|	3
Ефим	|	2
Дмитрий	|	2


Task #1. Weather
Assume you have five files and each file consists of two key/value pairs as in two columns in each file – a city name and its temperature recorded.
Here, name of city is the key and the temperature is value.
San Francisco, 22
Los Angeles, 15
Vancouver, 30
London, 25
Los Angeles, 16
Vancouver, 28
London,12

It is important to note that each file may consist of the data for same city multiple times.
Now, out of this data, we need to calculate the maximum temperature for each city across these five files. 

####################################

Task #2. Music
We have an online music website where users listen to various tracks, the data gets collected like shown below. Write a map reduce program to get following stats:
Number of times the track was shared with others
Number of times the track was listened to on the radio
Number of times the track was listened to in total
Number of times the track was skipped on the radio

The data looks like as shown below:
UserId|TrackId|Shared|Radio|Skip
111115|222|0|1|0
111113|225|1|0|0
111117|223|0|1|1
111115|225|1|0|0

```
import sys

tr_stat = {}

for row in sys.stdin:
    values = row.split('|')
    if len(values) == 5:
        tr_id = values[1]
        try:
            tr_stat[tr_id]['total'] += 1
            tr_stat[tr_id]['shared'] += int(values[2])
            tr_stat[tr_id]['radio'] += int(values[3])
            tr_stat[tr_id]['skip'] += int(values[4])
        except:
            tr_stat[tr_id] = dict()
            tr_stat[tr_id]['total'] = 1
            tr_stat[tr_id]['shared'] = int(values[2])
            tr_stat[tr_id]['radio'] = int(values[3])
            tr_stat[tr_id]['skip'] = int(values[4])

from pprint import pprint
pprint(tr_stat)
```
cat data.csv | python3 hometask.py > result.json
####################################

Task #3. DNA Counting
The exercise is to implement a kmer counter using hadoop. Conceptually this is very similar to the wordcount program, but since there are no spaces in the human genome, you have to count 3-mers overlapping kmers instead of discrete words.
The idea is if the genome is: ACACACAGT

####################################

Task #4. Find common friends
Instagram has a list of followers (note that followers are not bi-directional thing on Instagram.
If I follow you, does not mean you follow me). They also have lots of disk space and they serve hundreds of millions of requests everyday.
They've decided to pre-compute calculations when they can to reduce the processing time of requests.
One common processing request is the "You and Joe have 230 friends in common" feature.
When you visit someone's profile, you see a list of friends that you have in common.
This list doesn't change frequently so it'd be wasteful to recalculate it every time you visited the profile (sure you could use a decent caching strategy,
 but then I wouldn't be able to continue writing about mapreduce for this problem).
We're going to use mapreduce so that we can calculate everyone's common friends once a day and store those results.
Later on it's just a quick lookup. We've got lots of disk, it's cheap.
Assume the friends are stored as Person->[List of Friends], our friends list is then:
A -> B C D
B -> A C D E
C -> A B D E
D -> A B C E
E -> B C D

It is necessary to find common friends between all all pairs of friends.


