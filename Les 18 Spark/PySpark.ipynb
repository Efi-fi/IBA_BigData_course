{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName('hello').setMaster('spark://169.92.120.134:7077')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile(\"mydata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a SparkSession object to work with dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata_csv = spark.read.format('com.databricks.spark.csv')\\\n",
    "              .options(header='true', delimiter=',')\\\n",
    "              .load('hdfs:///opt/kris/mydata.csv')\n",
    "mydata_parquet = spark.read.parquet('hdfs:///opt/kris/mydata.parquet')\n",
    "mydata_db = spark.read.jdbc(url=url,table='testdb.employee',properties=db_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "columns = ['employee_name', 'is_chief', 'current_salary', 'desired_salary']\n",
    "vals = [\n",
    "     ('Christina', False, 100, 1000),\n",
    "     ('Olya', False, 100, 500),\n",
    "     ('Polina', False, 100, 300),\n",
    "     ('Misha', False, 100, None),\n",
    "     ('Mikalai', True, 10000, 20000),\n",
    "     ('Artyom', False, 100, 600)\n",
    "]\n",
    "\n",
    "salary_wish_df = spark.createDataFrame(vals, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulating data - built-in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_delta = salary_wish_df.filter(col('desired_salary').isNotNull())\\\n",
    "                             .withColumn('delta', when(not col('is_chief'), col('desired_salary') - col('current_salary')))\n",
    "\n",
    "delta_sum = salary_delta.groupBy(col('employee_name')).sum('delta').collect()[0][0]\n",
    "salary_delta_all = salary_delta.withColumn('delta', when(col('is_chief'), -delta_sum)\n",
    "                                                    .otherwise(col('delta')))\n",
    "final_salary = salary_delta_all.withColumn('final_salary', col('current_salary') + col('delta'))\n",
    "final_salary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulating data - sql query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_wish_df.registerTempTable(\"salary_wish_df\")\n",
    "salary_delta = sqlContext.sql(\"select df.*, (select desired_salary-current_salary\\\n",
    "                                             from salary_wish_df dfnc where is_chief is false\\\n",
    "                                             and dfnc.employee_name=df.employee_name) as delta\\\n",
    "                              from salary_wish_df as df\")\n",
    "salary_delta.createOrReplaceTempView(\"salary_delta\")\n",
    "delta_sum = sqlContext.sql(\"select sum(delta) from salary_delta\").collect()[0][0]\n",
    "salary_delta_all = sqlContext.sql(\"select df.*, case \\\n",
    "                                                when is_chief is true then \" + -delta_sum + \",\\\n",
    "                                                else delta\\\n",
    "                                                end delta\\\n",
    "                                                from salary_delta\")\n",
    "salary_delta_all.createOrReplaceTempView(\"salary_delta_all\")\n",
    "final_salary = sqlContext.sql(\"select df.*, current_salary + delta as final_salary from salary_delta_all\")\n",
    "final_salary.show()                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"New advanced name\")\n",
    "conf.set(\"spark.ui.port\", \"5001\")\n",
    "sc = SparcContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring Spark in the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --master ip --executor-cores=3 --diver 8G sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_delta = salary_wish_df.filter(col('desired_salary').isNotNull())\\\n",
    "                             .withColumn('delta', when(not col('is_chief'), col('desired_salary') - col('current_salary')))\n",
    "\n",
    "salary_delta.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing memory fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.storage.memoryFraction\", \"0.5\")\n",
    "conf.set(\"spark.shuffle.memoryFraction\", \"0.2\")\n",
    "sc = SparcContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_reduced = rdd.reduceByKey(_ + _, numPartitions = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = mydata_csv.filter(lambda line: line['col1'] == \"value1\")\n",
    "filtered_data.coalesce(5).cache()\n",
    "filtered_data.partitions().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
