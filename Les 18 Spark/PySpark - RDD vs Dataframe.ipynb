{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd_raw = sc.textFile('testdata.csv')\n",
    "print(type(rdd_raw))\n",
    "header = rdd_raw.first() #extract header\n",
    "rdd = rdd_raw.filter(lambda row: row != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "dataframe = spark.read.format('com.databricks.spark.csv')\\\n",
    "              .options(header='true', delimiter=',')\\\n",
    "              .load('testdata.csv')\n",
    "print(type(dataframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2/09 6:17,Product1,1200,Mastercard,carolina,Basildon,England,United Kingdom,1/2/09 6:00,1/2/09 6:08,51.5,-1.1166667\n",
      "1/2/09 4:53,Product1,1200,Visa,Betina,Parkville                   ,MO,United States,1/2/09 4:42,1/2/09 7:49,39.195,-94.68194\n",
      "1/2/09 13:08,Product1,1200,Mastercard,Federica e Andrea,Astoria                     ,OR,United States,1/1/09 16:21,1/3/09 12:32,46.18806,-123.83\n",
      "1/3/09 14:44,Product1,1200,Visa,Gouya,Echuca,Victoria,Australia,9/25/05 21:13,1/3/09 14:22,-36.1333333,144.75\n",
      "1/4/09 12:56,Product2,3600,Visa,Gerd W ,Cahaba Heights              ,AL,United States,11/15/08 15:47,1/4/09 12:45,33.52056,-86.8025\n",
      "1/4/09 13:19,Product1,1200,Visa,LAURENCE,Mickleton                   ,NJ,United States,9/24/08 15:19,1/4/09 13:04,39.79,-75.23806\n",
      "1/4/09 20:11,Product1,1200,Mastercard,Fleur,Peoria                      ,IL,United States,1/3/09 9:38,1/4/09 19:45,40.69361,-89.58889\n",
      "1/2/09 20:09,Product1,1200,Mastercard,adam,Martin                      ,TN,United States,1/2/09 17:43,1/4/09 20:01,36.34333,-88.85028\n",
      "1/4/09 13:17,Product1,1200,Mastercard,Renee Elisabeth,Tel Aviv,Tel Aviv,Israel,1/4/09 13:03,1/4/09 22:10,32.0666667,34.7666667\n",
      "1/4/09 14:11,Product1,1200,Visa,Aidan,Chatou,Ile-de-France,France,6/3/08 4:22,1/5/09 1:17,48.8833333,2.15\n"
     ]
    }
   ],
   "source": [
    "for row in rdd.take(10):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+-----+------------+-----------------+--------------------+-------------+--------------+---------------+------------+-----------+----------+\n",
      "|Transaction_date| Product|Price|Payment_Type|             Name|                City|        State|       Country|Account_Created|  Last_Login|   Latitude| Longitude|\n",
      "+----------------+--------+-----+------------+-----------------+--------------------+-------------+--------------+---------------+------------+-----------+----------+\n",
      "|     1/2/09 6:17|Product1| 1200|  Mastercard|         carolina|            Basildon|      England|United Kingdom|    1/2/09 6:00| 1/2/09 6:08|       51.5|-1.1166667|\n",
      "|     1/2/09 4:53|Product1| 1200|        Visa|           Betina|Parkville        ...|           MO| United States|    1/2/09 4:42| 1/2/09 7:49|     39.195| -94.68194|\n",
      "|    1/2/09 13:08|Product1| 1200|  Mastercard|Federica e Andrea|Astoria          ...|           OR| United States|   1/1/09 16:21|1/3/09 12:32|   46.18806|   -123.83|\n",
      "|    1/3/09 14:44|Product1| 1200|        Visa|            Gouya|              Echuca|     Victoria|     Australia|  9/25/05 21:13|1/3/09 14:22|-36.1333333|    144.75|\n",
      "|    1/4/09 12:56|Product2| 3600|        Visa|          Gerd W |Cahaba Heights   ...|           AL| United States| 11/15/08 15:47|1/4/09 12:45|   33.52056|  -86.8025|\n",
      "|    1/4/09 13:19|Product1| 1200|        Visa|         LAURENCE|Mickleton        ...|           NJ| United States|  9/24/08 15:19|1/4/09 13:04|      39.79| -75.23806|\n",
      "|    1/4/09 20:11|Product1| 1200|  Mastercard|            Fleur|Peoria           ...|           IL| United States|    1/3/09 9:38|1/4/09 19:45|   40.69361| -89.58889|\n",
      "|    1/2/09 20:09|Product1| 1200|  Mastercard|             adam|Martin           ...|           TN| United States|   1/2/09 17:43|1/4/09 20:01|   36.34333| -88.85028|\n",
      "|    1/4/09 13:17|Product1| 1200|  Mastercard|  Renee Elisabeth|            Tel Aviv|     Tel Aviv|        Israel|   1/4/09 13:03|1/4/09 22:10| 32.0666667|34.7666667|\n",
      "|    1/4/09 14:11|Product1| 1200|        Visa|            Aidan|              Chatou|Ile-de-France|        France|    6/3/08 4:22| 1/5/09 1:17| 48.8833333|      2.15|\n",
      "+----------------+--------+-----+------------+-----------------+--------------------+-------------+--------------+---------------+------------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_date: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Payment_Type: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Account_Created: string (nullable = true)\n",
      " |-- Last_Login: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "rdd_filtered = rdd.filter(lambda row_str: 'Product1' not in row_str)\n",
    "print(rdd_filtered.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dataframe_filtered = dataframe.filter(col(\"Product\")!=\"Product1\")\n",
    "print(dataframe_filtered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping by multiple columns + calculating average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1/2/09 6:17', 'Product1', 1200.0, 'Mastercard')\n",
      "('1/2/09 4:53', 'Product1', 1200.0, 'Visa')\n",
      "('1/2/09 13:08', 'Product1', 1200.0, 'Mastercard')\n",
      "('1/3/09 14:44', 'Product1', 1200.0, 'Visa')\n",
      "('1/4/09 12:56', 'Product2', 3600.0, 'Visa')\n",
      "('1/4/09 13:19', 'Product1', 1200.0, 'Visa')\n",
      "('1/4/09 20:11', 'Product1', 1200.0, 'Mastercard')\n",
      "('1/2/09 20:09', 'Product1', 1200.0, 'Mastercard')\n",
      "('1/4/09 13:17', 'Product1', 1200.0, 'Mastercard')\n",
      "('1/4/09 14:11', 'Product1', 1200.0, 'Visa')\n"
     ]
    }
   ],
   "source": [
    "rdd_split = rdd.map(lambda x: x.split(\",\"))\\\n",
    "               .map(lambda x: (x[0], x[1], float(x[2]), x[3]))\n",
    "for row in rdd_split.take(10):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Product1', 'Diners'), 1200.0)\n",
      "(('Product1', 'Amex'), 1202.2727272727273)\n",
      "(('Product2', 'Amex'), 3600.0)\n",
      "(('Product2', 'Diners'), 3600.0)\n",
      "(('Product3', 'Diners'), 7500.0)\n",
      "(('Product3', 'Amex'), 7500.0)\n",
      "(('Product1', 'Mastercard'), 1204.0772532188842)\n",
      "(('Product1', 'Visa'), 1224.3820224719102)\n",
      "(('Product2', 'Visa'), 3600.0)\n",
      "(('Product2', 'Mastercard'), 3600.0)\n",
      "(('Product3', 'Mastercard'), 7500.0)\n",
      "(('Product3 ', 'Mastercard'), 7500.0)\n",
      "(('Product3', 'Visa'), 7500.0)\n"
     ]
    }
   ],
   "source": [
    "test = rdd_split.map(lambda x: ((x[1], x[3]), (x[2],1)))\\\n",
    "         .reduceByKey(lambda a, b: tuple(x + y for x, y in zip(a,b)))\\\n",
    "         .map(lambda x: (x[0], (x[1][0]/x[1][1])))\n",
    "for row in test.take(100):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------------------+\n",
      "|  Product|Payment_type|        avg(Price)|\n",
      "+---------+------------+------------------+\n",
      "| Product3|        Amex|            7500.0|\n",
      "| Product1|  Mastercard|1204.0772532188842|\n",
      "| Product3|        Visa|            7500.0|\n",
      "| Product1|      Diners|            1200.0|\n",
      "| Product3|      Diners|            7500.0|\n",
      "| Product2|  Mastercard|            3600.0|\n",
      "|Product3 |  Mastercard|            7500.0|\n",
      "| Product2|      Diners|            3600.0|\n",
      "| Product2|        Visa|            3600.0|\n",
      "| Product2|        Amex|            3600.0|\n",
      "| Product1|        Amex|1202.2727272727273|\n",
      "| Product1|        Visa|1224.3820224719102|\n",
      "| Product3|  Mastercard|            7500.0|\n",
      "+---------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "dataframe.groupBy(\"Product\", \"Payment_type\").agg(avg(\"Price\")).show(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
